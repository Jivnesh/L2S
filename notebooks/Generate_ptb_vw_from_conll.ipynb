{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate VW format input files\n",
    "For train,test,dev files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Value of out of voc tag ', 1)\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "# Get all the tags in hash list\n",
    "hash = {}\n",
    "tag_file_name = '../Dep_ws/ptb_tags'\n",
    "# input_file_name = './sanskrit_treebank/ud_pos_ner_dp_train_san'\n",
    "files = ['../Dep_ws/ptb3.0-stanford.gold.cpos.test.conll']\n",
    "\n",
    "def readtags():\n",
    "    for line in open(tag_file_name).readlines():\n",
    "        hash[line.split()[0]] = int(line.strip().split()[1])\n",
    "tag_set = set()\n",
    "c = 1\n",
    "readtags()\n",
    "for input_file_name in files:\n",
    "    data = open(input_file_name).readlines()\n",
    "    writer = open(input_file_name+'.vw','w')\n",
    "    for line in data:\n",
    "        if line == '\\n':\n",
    "            writer.write('\\n')\n",
    "            continue\n",
    "        splits = line.strip().lower().split()\n",
    "        strw = \"|w %s\"%splits[1].replace(\":\",\"COL\");\n",
    "        strp = \"|p %s\"%splits[3].replace(\":\",\"COL\");\n",
    "        tag = splits[7]\n",
    "        tag_set.add(tag)\n",
    "        if tag not in hash:\n",
    "            hash[tag] = c\n",
    "            print(tag)\n",
    "            c+=1\n",
    "        writer.write('%s %s %s:%s%s %s\\n' % (int(splits[6]), hash[tag], int(splits[6]), tag, strw, strp))\n",
    "    writer.close()\n",
    "print('Value of out of voc tag ',c)\n",
    "print('successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j=1\n",
    "# for t in tag_set:\n",
    "#     print t, j\n",
    "#     j=j+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dict of MA because space is creating problem\n",
    "So change all tags to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "tag_dict = dict()\n",
    "for tag in tag_set:\n",
    "    tag_dict[tag] = i\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify evaluate script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: 86.720\n",
      "L: 84.313\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluation script modified from redshift parser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def pc(num, den):\n",
    "    return (num / float(den+1e-100)) * 100\n",
    "\n",
    "def fmt_acc(label, n, l_corr, u_corr, total_errs):\n",
    "    l_pc = pc(l_corr, n)\n",
    "    u_pc = pc(u_corr, n)\n",
    "    err_pc = pc(n - l_corr, total_errs)\n",
    "    return '%s\\t%d\\t%.3f\\t%.3f\\t%.3f' % (label, n, l_pc, u_pc, err_pc)\n",
    "\n",
    "\n",
    "def gen_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [Token(i, tok_str.split()) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "def gen_ptb_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [ptb_Token(i, tok_str.split()) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "class ptb_Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        self.label = attrs[-3]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-4])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-4])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        self.head = head\n",
    "        self.pos = attrs[3]\n",
    "        self.word = attrs[1]\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "        \n",
    "class Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        # CoNLL format\n",
    "        if len(attrs) == 10:\n",
    "            new_attrs = [str(int(attrs[0]) - 1)]\n",
    "            new_attrs.append(attrs[1])\n",
    "            new_attrs.append(attrs[3])\n",
    "            new_attrs.append(str(int(attrs[-4]) - 1))\n",
    "            new_attrs.append(attrs[-3])\n",
    "            attrs = new_attrs\n",
    "        self.label = attrs[-1]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-2])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-1])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        attrs.pop()\n",
    "        attrs.pop()\n",
    "        self.head = head\n",
    "        self.pos = attrs.pop()\n",
    "        self.word = attrs.pop()\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "\n",
    "test_loc = '../Dep_ws/dep.test.parse'\n",
    "gold_loc = '../Dep_ws/ptb3.0-stanford.gold.cpos.test.conll'\n",
    "n_by_label = defaultdict(lambda: defaultdict(int))\n",
    "u_by_label = defaultdict(lambda: defaultdict(int))\n",
    "l_by_label = defaultdict(lambda: defaultdict(int))\n",
    "N = 0\n",
    "u_nc = 0\n",
    "l_nc = 0\n",
    "for (sst, t), (ss, g) in zip(gen_toks(test_loc), gen_ptb_toks(gold_loc)):\n",
    "#     not eval_punct and \n",
    "    if g.word in \",.-;:'\\\"!?`{}()[]\":\n",
    "        continue\n",
    "    prev_g = g\n",
    "    prev_t = t\n",
    "    u_c = g.head == t.head\n",
    "    l_c = u_c and g.label.lower() == t.label.lower()\n",
    "    N += 1\n",
    "    l_nc += l_c\n",
    "    u_nc += u_c\n",
    "    n_by_label[g.dir][g.label] += 1\n",
    "    u_by_label[g.dir][g.label] += u_c\n",
    "    l_by_label[g.dir][g.label] += l_c\n",
    "n_l_err = N - l_nc\n",
    "for D in ['L', 'R']:\n",
    "    n_other = 0\n",
    "    l_other = 0\n",
    "    u_other = 0\n",
    "    for label, n in sorted(n_by_label[D].items(), key=lambda i: i[1], reverse=True):\n",
    "        if n == 0:\n",
    "            continue\n",
    "        elif n < 100:\n",
    "            n_other += n\n",
    "            l_other += l_by_label[D][label]\n",
    "            u_other += u_by_label[D][label]\n",
    "        else:\n",
    "            l_corr = l_by_label[D][label]\n",
    "            u_corr = u_by_label[D][label]\n",
    "print 'U: %.3f' % pc(u_nc, N)\n",
    "print 'L: %.3f' % pc(l_nc, N)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py2.7",
   "language": "python",
   "name": "py2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
