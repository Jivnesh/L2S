{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate VW format input files\n",
    "For train,test,dev files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"../data/cc.sanskrit.350.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of out of voc tag  1\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "# Get all the tags in hash list\n",
    "hash = {}\n",
    "tag_file_name = '../data/san_tags'\n",
    "# input_file_name = './sanskrit_treebank/ud_pos_ner_dp_train_san'\n",
    "files = ['../data/ud_pos_ner_dp_dev_san']\n",
    "\n",
    "def readtags():\n",
    "    for line in open(tag_file_name).readlines():\n",
    "        hash[line.split()[0]] = int(line.strip().split()[1])\n",
    "c = 1\n",
    "readtags()\n",
    "for input_file_name in files:\n",
    "    data = open(input_file_name).readlines()\n",
    "    writer = open('../models/Dep_san/ud_pos_ner_dp_dev_san'+'.vw','w')\n",
    "    for line in data:\n",
    "    #     print('Line',line)\n",
    "        if line == '\\n':\n",
    "            writer.write('\\n')\n",
    "            continue\n",
    "        splits = line.strip().split('\\t')\n",
    "\n",
    "        # This replace can be removed.\n",
    "        strw = \"|w %s\"%splits[1].replace(\":\",\"COL\");\n",
    "        strp = \"|p %s\"%splits[2].replace(\":\",\"COL\");\n",
    "        word = model.get_word_vector(splits[1]);\n",
    "        feature = ''\n",
    "        i=0\n",
    "        for v in word:\n",
    "            feature = feature+' '+'F'+str(i)+':'+str(v)\n",
    "            i=i+1\n",
    "        embedding = \"|FastText%s\"%feature;\n",
    "        tag = splits[-1]\n",
    "        if tag not in hash:\n",
    "            hash[tag] = c\n",
    "            c+=1\n",
    "\n",
    "        #writer.write('%s 1.0  %s:%s%s %s\\n'%((int(splits[7])+1) + (hash[tag]<<8), int(splits[7]),tag,strw, strp))\n",
    "        # Writing in order\n",
    "        # head, tag_integer mapping, head, tag, word, pos\n",
    "        writer.write('%s %s %s:%s%s %s %s\\n' % (int(splits[4]), hash[tag], int(splits[4]), tag, strw, strp,embedding))\n",
    "\n",
    "    writer.close()\n",
    "print('Value of out of voc tag ',c)\n",
    "print('successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dict of MA because space is creating problem\n",
    "So change all tags to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "tag_dict = dict()\n",
    "for tag in tag_set:\n",
    "    tag_dict[tag] = i\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify evaluate script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: 68.517\n",
      "L: 49.289\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluation script modified from redshift parser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def pc(num, den):\n",
    "    return (num / float(den+1e-100)) * 100\n",
    "\n",
    "def fmt_acc(label, n, l_corr, u_corr, total_errs):\n",
    "    l_pc = pc(l_corr, n)\n",
    "    u_pc = pc(u_corr, n)\n",
    "    err_pc = pc(n - l_corr, total_errs)\n",
    "    return '%s\\t%d\\t%.3f\\t%.3f\\t%.3f' % (label, n, l_pc, u_pc, err_pc)\n",
    "\n",
    "\n",
    "def gen_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [Token(i, tok_str.split()) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "def gen_note_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [note_Token(i, tok_str.split('\\t')) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "class note_Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        self.label = attrs[-1]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-2])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-1])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        self.head = head\n",
    "        self.pos = attrs[2]\n",
    "        self.word = attrs[1]\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "        \n",
    "class Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        # CoNLL format\n",
    "        if len(attrs) == 10:\n",
    "            new_attrs = [str(int(attrs[0]) - 1)]\n",
    "            new_attrs.append(attrs[1])\n",
    "            new_attrs.append(attrs[3])\n",
    "            new_attrs.append(str(int(attrs[-4]) - 1))\n",
    "            new_attrs.append(attrs[-3])\n",
    "            attrs = new_attrs\n",
    "        self.label = attrs[-1]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-2])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-1])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        attrs.pop()\n",
    "        attrs.pop()\n",
    "        self.head = head\n",
    "        self.pos = attrs.pop()\n",
    "        self.word = attrs.pop()\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "\n",
    "test_loc = 'Dep_san/dep.test.parse'\n",
    "gold_loc = 'sanskrit_treebank/ud_pos_ner_dp_dev_san'\n",
    "n_by_label = defaultdict(lambda: defaultdict(int))\n",
    "u_by_label = defaultdict(lambda: defaultdict(int))\n",
    "l_by_label = defaultdict(lambda: defaultdict(int))\n",
    "N = 0\n",
    "u_nc = 0\n",
    "l_nc = 0\n",
    "for (sst, t), (ss, g) in zip(gen_toks(test_loc), gen_note_toks(gold_loc)):\n",
    "#     not eval_punct and \n",
    "    if g.word in \",.-;:'\\\"!?`{}()[]\":\n",
    "        continue\n",
    "    prev_g = g\n",
    "    prev_t = t\n",
    "    u_c = g.head == t.head\n",
    "    l_c = u_c and g.label.lower() == t.label.lower()\n",
    "    N += 1\n",
    "    l_nc += l_c\n",
    "    u_nc += u_c\n",
    "    n_by_label[g.dir][g.label] += 1\n",
    "    u_by_label[g.dir][g.label] += u_c\n",
    "    l_by_label[g.dir][g.label] += l_c\n",
    "n_l_err = N - l_nc\n",
    "for D in ['L', 'R']:\n",
    "    n_other = 0\n",
    "    l_other = 0\n",
    "    u_other = 0\n",
    "    for label, n in sorted(n_by_label[D].items(), key=lambda i: i[1], reverse=True):\n",
    "        if n == 0:\n",
    "            continue\n",
    "        elif n < 100:\n",
    "            n_other += n\n",
    "            l_other += l_by_label[D][label]\n",
    "            u_other += u_by_label[D][label]\n",
    "        else:\n",
    "            l_corr = l_by_label[D][label]\n",
    "            u_corr = u_by_label[D][label]\n",
    "print 'U: %.3f' % pc(u_nc, N)\n",
    "print 'L: %.3f' % pc(l_nc, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '2', '6:karwa|w', 'aham', '|p', 'nom.', 'sg.', '*']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = '6 2 6:karwa|w aham |p nom. sg. *'\n",
    "r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
