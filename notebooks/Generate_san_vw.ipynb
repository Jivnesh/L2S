{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate_san_vw\n",
    "For train,test,dev files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"../../fastText/result/cc.sanskrit.50.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeclinable = ['ind','prep','interj','prep','conj','part','abs','ca abs']\n",
    "case_list = ['nom','voc','acc','inst','dat','abl','g','loc']\n",
    "gender_list = ['n','f','m','*']\n",
    "person_list = ['1','2','3']\n",
    "no_list = ['du','sg','pl']\n",
    "pops = [' ac',' ps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of out of voc tag  1\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "# Get all the tags in hash list\n",
    "import re \n",
    "hash = {}\n",
    "tag_file_name = '../data/san_tags'\n",
    "# input_file_name = './sanskrit_treebank/ud_pos_ner_dp_train_san'\n",
    "files = ['ud_pos_ner_dp_dev_san','ud_pos_ner_dp_train_san','ud_pos_ner_dp_test_san']\n",
    "\n",
    "def readtags():\n",
    "    for line in open(tag_file_name).readlines():\n",
    "        hash[line.split()[0]] = int(line.strip().split()[1])\n",
    "c = 1\n",
    "readtags()\n",
    "for input_file_name in files:\n",
    "    data = open('../data/'+input_file_name).readlines()\n",
    "    writer = open('../models/Dep_san_temp/'+input_file_name+'.vw','w')\n",
    "    for line in data:\n",
    "    #     print('Line',line)\n",
    "        if line == '\\n':\n",
    "            writer.write('\\n')\n",
    "            continue\n",
    "        splits = line.strip().split('\\t')\n",
    "\n",
    "        # This replace can be removed.\n",
    "        strw = \"|w %s\"%splits[3].replace(\":\",\"COL\");\n",
    "        strp = \"|p %s\"%splits[2].replace(\":\",\"COL\");\n",
    "#         embedding = \"|vec %s\"%splits[1].replace(\":\",\"COL\");\n",
    "        \n",
    "#         #############################################\n",
    "#         # Variation : 0(a)\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         strp = \"|p %s\"%temp.replace(\":\",\"COL\");\n",
    "        \n",
    "#         #############################################\n",
    "#         # Variation : I\n",
    "#         # Variation : II added c=c.replace(' ac','').replace(' ps','')\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         temp = temp.split('.')\n",
    "#         if temp[-1] == '':\n",
    "#             temp.pop(-1)\n",
    "#         if len(temp) == 3: \n",
    "#             var = 'case=%s no=%s gender=%s'%(temp[0].strip().replace(' ac','').replace(' ps',''),temp[1].strip(),temp[2].strip())\n",
    "#         elif len(temp) == 1:\n",
    "#             var = 'case=%s'%(temp[0].strip().replace(' ac','').replace(' ps',''))\n",
    "#         elif len(temp) == 2:\n",
    "#             c=temp[0].strip()+ ' '+temp[1].strip()\n",
    "#             c=c.replace(' ac','').replace(' ps','')\n",
    "#             var = 'case=%s'%(c)\n",
    "#         elif len(temp) == 4:\n",
    "#             c=temp[0].strip()+ ' '+temp[1].strip()\n",
    "#             c=c.replace(' ac','').replace(' ps','')\n",
    "#             var = 'case=%s no=%s gender=%s'%(c,temp[2].strip(),temp[3].strip())\n",
    "#         elif len(temp) == 5:\n",
    "#             c=temp[0].strip()+ ' '+temp[1].strip() + ' '+temp[2].strip()\n",
    "#             c=c.replace(' ac','').replace(' ps','')\n",
    "#             var = 'case=%s no=%s gender=%s'%(c,temp[3].strip(),temp[4].strip())\n",
    "#         else:\n",
    "#             print('Handle me!')\n",
    "#             print(temp)\n",
    "#         strp = \"|pos %s\"%var;\n",
    "        #################################################\n",
    "#         # Variation:III\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         temp = temp.split('.')\n",
    "#         if temp[-1] == '':\n",
    "#             temp.pop(-1)\n",
    "#         # Remove active passive\n",
    "#         case=''\n",
    "#         no=''\n",
    "#         person=''\n",
    "#         gender=''\n",
    "#         tense=''\n",
    "        \n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b in pops:\n",
    "#                 temp.pop(a)\n",
    "#         # Get gender\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in gender_list:\n",
    "#                 gender = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get case\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in case_list:\n",
    "#                 case = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get person\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in person_list:\n",
    "#                 person = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get no\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in no_list:\n",
    "#                 no = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get Tense\n",
    "#         for b in temp:\n",
    "#             tense=tense+ ' '+b.strip()\n",
    "#         tense=tense.strip()\n",
    "#         var=''\n",
    "#         if case:\n",
    "#             t = 'case=%s '%case\n",
    "#             var=var+t\n",
    "#         if gender:\n",
    "#             t = 'gender=%s '%gender\n",
    "#             var=var+t\n",
    "#         if tense:\n",
    "#             t = 'tense=%s '%tense\n",
    "#             var=var+t\n",
    "#         if no:\n",
    "#             t = 'no=%s '%no\n",
    "#             var=var+t\n",
    "#         if person:\n",
    "#             t = 'case=%s '%person\n",
    "#             var=var+t\n",
    "# #         print(var)\n",
    "#         strp = \"|pos %s\"%var.strip();\n",
    "#         ##################################################################################\n",
    "#         # Variation IV\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         temp = temp.split('.')\n",
    "#         if temp[-1] == '':\n",
    "#             temp.pop(-1)\n",
    "#         # Remove active passive\n",
    "#         case=''\n",
    "#         no=''\n",
    "#         person=''\n",
    "#         gender=''\n",
    "#         tense=''\n",
    "#         coarse=''\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b in pops:\n",
    "#                 temp.pop(a)\n",
    "#         # Get gender\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in gender_list:\n",
    "#                 gender = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get case\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in case_list:\n",
    "#                 case = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         if case!= '':\n",
    "#             coarse ='Noun'\n",
    "#         # Get person\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in person_list:\n",
    "#                 person = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get no\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in no_list:\n",
    "#                 no = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get Tense\n",
    "        \n",
    "#         for b in temp:\n",
    "#             tense=tense+ ' '+b.strip()\n",
    "#         tense=tense.strip()\n",
    "#         if tense == 'adv':\n",
    "#             coarse = 'adv'\n",
    "#         for ind in indeclinable:\n",
    "#             if tense == ind:\n",
    "#                 coarse = 'Ind'\n",
    "#         if tense!='' and coarse=='':\n",
    "#             if no=='':\n",
    "#                 coarse = 'IV'\n",
    "#             elif person !='' and no!='':\n",
    "#                 coarse = 'FV'\n",
    "#         var=''\n",
    "#         if case:\n",
    "#             t = 'case=%s '%case\n",
    "#             var=var+t\n",
    "#         if gender:\n",
    "#             t = 'gender=%s '%gender\n",
    "#             var=var+t\n",
    "#         if tense:\n",
    "#             t = 'tense=%s '%tense\n",
    "#             var=var+t\n",
    "#         if no:\n",
    "#             t = 'no=%s '%no\n",
    "#             var=var+t\n",
    "#         if person:\n",
    "#             t = 'case=%s '%person\n",
    "#             var=var+t\n",
    "#         if coarse:\n",
    "#             t = 'coarse=%s '%coarse\n",
    "#             var=var+t\n",
    "#         strp = \"|pos %s\"%var.strip();\n",
    "# #         print(var)\n",
    "        ###############################################################\n",
    "#         # Use word embedding\n",
    "#         feature = ''\n",
    "#         j=0\n",
    "#         for v in transformed_matrix[word_bag.index(splits[1])]:\n",
    "#             feature = feature+' '+'F'+str(j)+':'+str(v)\n",
    "#             j+=1\n",
    "#         embedding = \"|FastText%s\"%feature;\n",
    "#         word = model.get_word_vector(splits[1]);\n",
    "#         feature = ''\n",
    "#         i=0\n",
    "#         for v in word[:20]:\n",
    "#             feature = feature+' '+str(v)\n",
    "# #             feature = feature+' '+'F'+str(i)+':'+str(v)\n",
    "\n",
    "#             i=i+1\n",
    "#         embedding = \"|FastText%s\"%feature;\n",
    "#         strq = \"|FASTTEXT T0:%s U1:%s\"%(word[1],word[1]);\n",
    "        tag = splits[-1]\n",
    "        if tag not in hash:\n",
    "            hash[tag] = c\n",
    "            c+=1\n",
    "\n",
    "        #writer.write('%s 1.0  %s:%s%s %s\\n'%((int(splits[7])+1) + (hash[tag]<<8), int(splits[7]),tag,strw, strp))\n",
    "        # Writing in order\n",
    "        # head, tag_integer mapping, head, tag, word, pos\n",
    "        writer.write('%s %s %s:%s%s %s\\n' % (int(splits[4]), hash[tag], int(splits[4]), tag, strw, strp))\n",
    "\n",
    "    writer.close()\n",
    "print('Value of out of voc tag ',c)\n",
    "print('successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26397"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tag_file_name = '../data/san_tags'\n",
    "word_bag = []\n",
    "orginal_matrix = np.zeros((26397,50))\n",
    "i=0\n",
    "files = ['ud_pos_ner_dp_dev_san','ud_pos_ner_dp_train_san','ud_pos_ner_dp_test_san']\n",
    "for input_file_name in files:\n",
    "    data = open('../data/'+input_file_name).readlines()\n",
    "    for line in data:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        splits = line.strip().split('\\t')\n",
    "        orginal_matrix[i,:]= model.get_word_vector(splits[1]);\n",
    "        i=i+1\n",
    "        word_text = splits[1]\n",
    "        word_bag.append(word_text)\n",
    "len(word_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "dimension = 5\n",
    "pca = PCA(n_components=dimension)\n",
    "transformed_matrix = pca.fit_transform(orginal_matrix)\n",
    "# pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03731635, -0.59988051, -0.44631659, ...,  0.3015213 ,\n",
       "        -0.30134972, -0.24681702],\n",
       "       [ 1.53255887,  0.33916621, -0.12257588, ..., -0.09277995,\n",
       "        -0.39587206,  0.13557693],\n",
       "       [-0.65364841, -1.40964452,  0.25741981, ...,  0.01773374,\n",
       "        -0.06623374,  0.1217984 ],\n",
       "       ...,\n",
       "       [-0.04431626, -1.06378246,  0.05621747, ...,  0.11447453,\n",
       "        -0.45325464, -0.93442413],\n",
       "       [-0.47157684, -1.01997806,  0.10494299, ...,  0.21324203,\n",
       "        -0.53294866, -0.11717879],\n",
       "       [-0.26982698, -0.57823036, -0.13937365, ...,  0.55552532,\n",
       "         0.7673041 ,  0.58197453]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "writer = open('../models/Dep_san/'+'embedding_size_'+str(dimension)+'.dict','w')\n",
    "for i,word in enumerate(word_bag):\n",
    "    feature = ''\n",
    "    j=0\n",
    "    for v in transformed_matrix[i]:\n",
    "        feature = feature+' '+'F'+str(j)+':'+str(v)\n",
    "        j+=1\n",
    "    embedding = word+\"%s\"%feature;\n",
    "    writer.write('%s\\n' % (embedding))\n",
    "writer.close()\n",
    "print('successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "# tag_file_name = '../data/san_tags'\n",
    "# files = ['ud_pos_ner_dp_dev_san','ud_pos_ner_dp_train_san','ud_pos_ner_dp_test_san']\n",
    "# writer = open('../models/Dep_san/'+'embedding.dict','w')\n",
    "# for input_file_name in files:\n",
    "#     data = open('../data/'+input_file_name).readlines()\n",
    "#     for line in data:\n",
    "#         if line == '\\n':\n",
    "#             continue\n",
    "#         splits = line.strip().split('\\t')\n",
    "#         word = model.get_word_vector(splits[1]);\n",
    "#         feature = ''\n",
    "#         i=0\n",
    "#         for v in word:\n",
    "#             feature = feature+' '+'F'+str(i)+':'+str(v)\n",
    "#             i+=1\n",
    "#         embedding = splits[1]+\"%s\"%feature;\n",
    "     \n",
    "#         writer.write('%s\\n' % (embedding))\n",
    "\n",
    "# writer.close()\n",
    "# print('successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dict of MA because space is creating problem\n",
    "So change all tags to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "tag_dict = dict()\n",
    "for tag in tag_set:\n",
    "    tag_dict[tag] = i\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify evaluate script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: 68.517\n",
      "L: 49.289\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluation script modified from redshift parser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def pc(num, den):\n",
    "    return (num / float(den+1e-100)) * 100\n",
    "\n",
    "def fmt_acc(label, n, l_corr, u_corr, total_errs):\n",
    "    l_pc = pc(l_corr, n)\n",
    "    u_pc = pc(u_corr, n)\n",
    "    err_pc = pc(n - l_corr, total_errs)\n",
    "    return '%s\\t%d\\t%.3f\\t%.3f\\t%.3f' % (label, n, l_pc, u_pc, err_pc)\n",
    "\n",
    "\n",
    "def gen_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [Token(i, tok_str.split()) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "def gen_note_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [note_Token(i, tok_str.split('\\t')) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "class note_Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        self.label = attrs[-1]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-2])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-1])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        self.head = head\n",
    "        self.pos = attrs[2]\n",
    "        self.word = attrs[1]\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "        \n",
    "class Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        # CoNLL format\n",
    "        if len(attrs) == 10:\n",
    "            new_attrs = [str(int(attrs[0]) - 1)]\n",
    "            new_attrs.append(attrs[1])\n",
    "            new_attrs.append(attrs[3])\n",
    "            new_attrs.append(str(int(attrs[-4]) - 1))\n",
    "            new_attrs.append(attrs[-3])\n",
    "            attrs = new_attrs\n",
    "        self.label = attrs[-1]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-2])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-1])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        attrs.pop()\n",
    "        attrs.pop()\n",
    "        self.head = head\n",
    "        self.pos = attrs.pop()\n",
    "        self.word = attrs.pop()\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "\n",
    "test_loc = 'Dep_san/dep.test.parse'\n",
    "gold_loc = 'sanskrit_treebank/ud_pos_ner_dp_dev_san'\n",
    "n_by_label = defaultdict(lambda: defaultdict(int))\n",
    "u_by_label = defaultdict(lambda: defaultdict(int))\n",
    "l_by_label = defaultdict(lambda: defaultdict(int))\n",
    "N = 0\n",
    "u_nc = 0\n",
    "l_nc = 0\n",
    "for (sst, t), (ss, g) in zip(gen_toks(test_loc), gen_note_toks(gold_loc)):\n",
    "#     not eval_punct and \n",
    "    if g.word in \",.-;:'\\\"!?`{}()[]\":\n",
    "        continue\n",
    "    prev_g = g\n",
    "    prev_t = t\n",
    "    u_c = g.head == t.head\n",
    "    l_c = u_c and g.label.lower() == t.label.lower()\n",
    "    N += 1\n",
    "    l_nc += l_c\n",
    "    u_nc += u_c\n",
    "    n_by_label[g.dir][g.label] += 1\n",
    "    u_by_label[g.dir][g.label] += u_c\n",
    "    l_by_label[g.dir][g.label] += l_c\n",
    "n_l_err = N - l_nc\n",
    "for D in ['L', 'R']:\n",
    "    n_other = 0\n",
    "    l_other = 0\n",
    "    u_other = 0\n",
    "    for label, n in sorted(n_by_label[D].items(), key=lambda i: i[1], reverse=True):\n",
    "        if n == 0:\n",
    "            continue\n",
    "        elif n < 100:\n",
    "            n_other += n\n",
    "            l_other += l_by_label[D][label]\n",
    "            u_other += u_by_label[D][label]\n",
    "        else:\n",
    "            l_corr = l_by_label[D][label]\n",
    "            u_corr = u_by_label[D][label]\n",
    "print 'U: %.3f' % pc(u_nc, N)\n",
    "print 'L: %.3f' % pc(l_nc, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Tripple\n",
    "- From the training set, provide the following triples as a csv <br>\n",
    "    - head word case, child word case, relation <br>\n",
    "    - Please note that in case of entries with no case in it, give their coarse POS (if root, give as root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeclinable = ['ind','prep','interj','prep','conj','part','abs','ca abs']\n",
    "case_list = ['nom','voc','acc','i','inst','dat','abl','g','loc']\n",
    "gender_list = ['n','f','m','*']\n",
    "person_list = ['1','2','3']\n",
    "no_list = ['du','sg','pl']\n",
    "pops = [' ac',' ps']\n",
    "def get_case(ma):\n",
    "        temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", ma).replace('[] ','').strip(' []')\n",
    "        temp = temp.split('.')\n",
    "        if temp[-1] == '':\n",
    "            temp.pop(-1)\n",
    "        # Remove active passive\n",
    "        case=''\n",
    "        no=''\n",
    "        person=''\n",
    "        gender=''\n",
    "        tense=''\n",
    "        coarse=''\n",
    "        for a,b in enumerate(temp):\n",
    "            if b in pops:\n",
    "                temp.pop(a)\n",
    "        # Get gender\n",
    "        for a,b in enumerate(temp):\n",
    "            if b.strip() in gender_list:\n",
    "                gender = b.strip()\n",
    "                temp.pop(a)\n",
    "        # Get case\n",
    "        for a,b in enumerate(temp):\n",
    "            if b.strip() in case_list:\n",
    "                case = b.strip()\n",
    "                temp.pop(a)\n",
    "        if case!= '':\n",
    "            coarse ='Noun'\n",
    "        # Get person\n",
    "        for a,b in enumerate(temp):\n",
    "            if b.strip() in person_list:\n",
    "                person = b.strip()\n",
    "                temp.pop(a)\n",
    "        # Get no\n",
    "        for a,b in enumerate(temp):\n",
    "            if b.strip() in no_list:\n",
    "                no = b.strip()\n",
    "                temp.pop(a)\n",
    "        # Get Tense\n",
    "        \n",
    "        for b in temp:\n",
    "            tense=tense+ ' '+b.strip()\n",
    "        tense=tense.strip()\n",
    "        if tense == 'adv':\n",
    "            coarse = 'adv'\n",
    "        for ind in indeclinable:\n",
    "            if tense == ind:\n",
    "                coarse = 'Ind'\n",
    "        if tense!='' and coarse=='':\n",
    "            if person !='' and no!='':\n",
    "                coarse= 'FV'\n",
    "            else:\n",
    "                coarse = 'IV'\n",
    "        if case == 'i':\n",
    "            return 'inst'\n",
    "        if case !='':\n",
    "            return case\n",
    "        else:\n",
    "            return coarse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the tags in hash list\n",
    "import re \n",
    "import os\n",
    "import pandas as pd\n",
    "tripple = pd.DataFrame(columns=['child_case','head_case','relation','child_pos','head_pos','file_name'])\n",
    "for file in os.listdir('../data/train'):\n",
    "    data = pd.read_csv('../data/train/'+file,sep=',')\n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,6] != data.iloc[i,6]:\n",
    "            tripple = tripple.append({'child_case': get_case(data.iloc[i,3]),'head_case':\\\n",
    "                                  'ROOT','relation':'root','child_pos': data.iloc[i,3],\\\n",
    "                                   'head_pos': 'NA','file_name': file }, ignore_index=True)\n",
    "            continue\n",
    "        tripple = tripple.append({'child_case': get_case(data.iloc[i,3]),'head_case':\\\n",
    "                                  get_case(data.iloc[int(data.iloc[i,6]),3]),'relation':data.iloc[i,5],\\\n",
    "                                  'child_pos': data.iloc[i,3],\\\n",
    "                                   'head_pos': data.iloc[int(data.iloc[i,6]),3],'file_name': file}, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripple.to_csv('../files/tripples.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build coarse to MA dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_set = set()\n",
    "word = None\n",
    "with open('./pos_embedding_1_hot.475', 'r', encoding='utf-8') as f:\n",
    "    # skip first line\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        word, vec = line.split('@', 1)\n",
    "        ma_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeclinable = ['ind','prep','interj','prep','conj','part','abs','ca abs']\n",
    "case_list = ['nom','voc','acc','i','inst','dat','abl','g','loc']\n",
    "gender_list = ['n','f','m','*']\n",
    "person_list = ['1','2','3']\n",
    "no_list = ['du','sg','pl']\n",
    "pops = [' ac',' ps']\n",
    "import re\n",
    "store =[]\n",
    "coarse_to_ma_dict =dict()\n",
    "for ma in ma_set:\n",
    "    temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", ma).replace('[] ','').strip(' []')\n",
    "    temp = temp.split('.')\n",
    "    if temp[-1] == '':\n",
    "        temp.pop(-1)\n",
    "    # Remove active passive\n",
    "    case='-'\n",
    "    no='-'\n",
    "    person='-'\n",
    "    gender='-'\n",
    "    tense='-'\n",
    "    coarse='-'\n",
    "    for a,b in enumerate(temp):\n",
    "        if b in pops:\n",
    "            temp.pop(a)\n",
    "    # Get gender\n",
    "    for a,b in enumerate(temp):\n",
    "        if b.strip() in gender_list:\n",
    "            gender = b.strip()\n",
    "            temp.pop(a)\n",
    "    # Get case\n",
    "    for a,b in enumerate(temp):\n",
    "        if b.strip() in case_list:\n",
    "            case = b.strip()\n",
    "            temp.pop(a)\n",
    "    if case!= '-':\n",
    "        coarse ='Noun'\n",
    "    # Get person\n",
    "    for a,b in enumerate(temp):\n",
    "        if b.strip() in person_list:\n",
    "            person = b.strip()\n",
    "            temp.pop(a)\n",
    "    # Get no\n",
    "    for a,b in enumerate(temp):\n",
    "        if b.strip() in no_list:\n",
    "            no = b.strip()\n",
    "            temp.pop(a)\n",
    "    # Get Tense\n",
    "    flag = 0\n",
    "    for b in temp:\n",
    "        if flag == 0:\n",
    "            tense=''\n",
    "            flag=1\n",
    "        tense=tense+ ' '+b.strip()\n",
    "    tense=tense.strip()\n",
    "    if tense == 'adv':\n",
    "        coarse = 'adv'\n",
    "    for ind in indeclinable:\n",
    "        if tense == ind:\n",
    "            coarse = 'Ind'\n",
    "    if tense!='-' and coarse=='-':\n",
    "        if person !='-' and no!='-':\n",
    "            coarse = 'FV'\n",
    "        else:\n",
    "            coarse = 'IV'\n",
    "    if coarse not in coarse_to_ma_dict.keys():\n",
    "        coarse_to_ma_dict[coarse] = []\n",
    "    else:\n",
    "        coarse_to_ma_dict[coarse].append(ma)\n",
    "    store.append([case,gender,tense,no,person,coarse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_to_ma_dict\n",
    "import json\n",
    "with open('../files/coarse_to_ma_dict.json', 'w') as f:\n",
    "    json.dump(coarse_to_ma_dict, f)\n",
    "with open('../files/coarse_to_ma_dict.json', 'r') as fh:\n",
    "    jd1 = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IV', 'FV', 'Noun', 'Ind', 'adv'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_to_ma_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IV', 'FV', 'Noun', 'Ind', 'adv'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarse_to_ma_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
