{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate_san_vw\n",
    "For train,test,dev files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.load_model(\"../../fastText/result/cc.sanskrit.50.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeclinable = ['ind','prep','interj','prep','conj','part','abs','ca abs']\n",
    "case_list = ['nom','voc','acc','i','dat','abl','g','loc']\n",
    "gender_list = ['n','f','m','*']\n",
    "person_list = ['1','2','3']\n",
    "no_list = ['du','sg','pl']\n",
    "pops = [' ac',' ps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of out of voc tag  1\n",
      "successful\n"
     ]
    }
   ],
   "source": [
    "# Get all the tags in hash list\n",
    "import re \n",
    "hash = {}\n",
    "tag_file_name = '../data/san_tags'\n",
    "# input_file_name = './sanskrit_treebank/ud_pos_ner_dp_train_san'\n",
    "files = ['ud_pos_ner_dp_dev_san','ud_pos_ner_dp_train_san','ud_pos_ner_dp_test_san']\n",
    "\n",
    "def readtags():\n",
    "    for line in open(tag_file_name).readlines():\n",
    "        hash[line.split()[0]] = int(line.strip().split()[1])\n",
    "c = 1\n",
    "readtags()\n",
    "for input_file_name in files:\n",
    "    data = open('../data/'+input_file_name).readlines()\n",
    "    writer = open('../models/Dep_san/'+input_file_name+'.vw','w')\n",
    "    for line in data:\n",
    "    #     print('Line',line)\n",
    "        if line == '\\n':\n",
    "            writer.write('\\n')\n",
    "            continue\n",
    "        splits = line.strip().split('\\t')\n",
    "\n",
    "        # This replace can be removed.\n",
    "        strw = \"|w %s\"%splits[1].replace(\":\",\"COL\");\n",
    "        strp = \"|p %s\"%splits[2].replace(\":\",\"COL\");\n",
    "        embedding = \"|vec %s\"%splits[1].replace(\":\",\"COL\");\n",
    "        \n",
    "#         #############################################\n",
    "#         # Variation : 0(a)\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         strp = \"|p %s\"%temp.replace(\":\",\"COL\");\n",
    "        \n",
    "#         #############################################\n",
    "#         # Variation : I\n",
    "#         # Variation : II added c=c.replace(' ac','').replace(' ps','')\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         temp = temp.split('.')\n",
    "#         if temp[-1] == '':\n",
    "#             temp.pop(-1)\n",
    "#         if len(temp) == 3: \n",
    "#             var = 'case=%s no=%s gender=%s'%(temp[0].strip().replace(' ac','').replace(' ps',''),temp[1].strip(),temp[2].strip())\n",
    "#         elif len(temp) == 1:\n",
    "#             var = 'case=%s'%(temp[0].strip().replace(' ac','').replace(' ps',''))\n",
    "#         elif len(temp) == 2:\n",
    "#             c=temp[0].strip()+ ' '+temp[1].strip()\n",
    "#             c=c.replace(' ac','').replace(' ps','')\n",
    "#             var = 'case=%s'%(c)\n",
    "#         elif len(temp) == 4:\n",
    "#             c=temp[0].strip()+ ' '+temp[1].strip()\n",
    "#             c=c.replace(' ac','').replace(' ps','')\n",
    "#             var = 'case=%s no=%s gender=%s'%(c,temp[2].strip(),temp[3].strip())\n",
    "#         elif len(temp) == 5:\n",
    "#             c=temp[0].strip()+ ' '+temp[1].strip() + ' '+temp[2].strip()\n",
    "#             c=c.replace(' ac','').replace(' ps','')\n",
    "#             var = 'case=%s no=%s gender=%s'%(c,temp[3].strip(),temp[4].strip())\n",
    "#         else:\n",
    "#             print('Handle me!')\n",
    "#             print(temp)\n",
    "#         strp = \"|pos %s\"%var;\n",
    "        #################################################\n",
    "#         # Variation:III\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         temp = temp.split('.')\n",
    "#         if temp[-1] == '':\n",
    "#             temp.pop(-1)\n",
    "#         # Remove active passive\n",
    "#         case=''\n",
    "#         no=''\n",
    "#         person=''\n",
    "#         gender=''\n",
    "#         tense=''\n",
    "        \n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b in pops:\n",
    "#                 temp.pop(a)\n",
    "#         # Get gender\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in gender_list:\n",
    "#                 gender = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get case\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in case_list:\n",
    "#                 case = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get person\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in person_list:\n",
    "#                 person = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get no\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in no_list:\n",
    "#                 no = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get Tense\n",
    "#         for b in temp:\n",
    "#             tense=tense+ ' '+b.strip()\n",
    "#         tense=tense.strip()\n",
    "#         var=''\n",
    "#         if case:\n",
    "#             t = 'case=%s '%case\n",
    "#             var=var+t\n",
    "#         if gender:\n",
    "#             t = 'gender=%s '%gender\n",
    "#             var=var+t\n",
    "#         if tense:\n",
    "#             t = 'tense=%s '%tense\n",
    "#             var=var+t\n",
    "#         if no:\n",
    "#             t = 'no=%s '%no\n",
    "#             var=var+t\n",
    "#         if person:\n",
    "#             t = 'case=%s '%person\n",
    "#             var=var+t\n",
    "#         print(var)\n",
    "#         strp = \"|pos %s\"%var.strip();\n",
    "#         ##################################################################################\n",
    "#         # Variation IV\n",
    "#         temp = re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\\g<1>\\g<2>\", splits[2]).replace('[] ','').strip(' []')\n",
    "#         temp = temp.split('.')\n",
    "#         if temp[-1] == '':\n",
    "#             temp.pop(-1)\n",
    "#         # Remove active passive\n",
    "#         case=''\n",
    "#         no=''\n",
    "#         person=''\n",
    "#         gender=''\n",
    "#         tense=''\n",
    "#         coarse=''\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b in pops:\n",
    "#                 temp.pop(a)\n",
    "#         # Get gender\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in gender_list:\n",
    "#                 gender = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get case\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in case_list:\n",
    "#                 case = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         if case!= '':\n",
    "#             coarse ='Noun'\n",
    "#         # Get person\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in person_list:\n",
    "#                 person = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get no\n",
    "#         for a,b in enumerate(temp):\n",
    "#             if b.strip() in no_list:\n",
    "#                 no = b.strip()\n",
    "#                 temp.pop(a)\n",
    "#         # Get Tense\n",
    "        \n",
    "#         for b in temp:\n",
    "#             tense=tense+ ' '+b.strip()\n",
    "#         tense=tense.strip()\n",
    "#         if tense == 'adv':\n",
    "#             coarse = 'adv'\n",
    "#         for ind in indeclinable:\n",
    "#             if tense == ind:\n",
    "#                 coarse = 'Ind'\n",
    "#         if tense!='' and coarse=='':\n",
    "#             if no=='':\n",
    "#                 coarse = 'IV'\n",
    "#             elif person !='' and no!='':\n",
    "#                 coarse = 'FV'\n",
    "#         var=''\n",
    "#         if case:\n",
    "#             t = 'case=%s '%case\n",
    "#             var=var+t\n",
    "#         if gender:\n",
    "#             t = 'gender=%s '%gender\n",
    "#             var=var+t\n",
    "#         if tense:\n",
    "#             t = 'tense=%s '%tense\n",
    "#             var=var+t\n",
    "#         if no:\n",
    "#             t = 'no=%s '%no\n",
    "#             var=var+t\n",
    "#         if person:\n",
    "#             t = 'case=%s '%person\n",
    "#             var=var+t\n",
    "#         if coarse:\n",
    "#             t = 'coarse=%s '%coarse\n",
    "#             var=var+t\n",
    "#         strp = \"|pos %s\"%var.strip();\n",
    "# #         print(var)\n",
    "        ###############################################################\n",
    "        # Use word embedding\n",
    "        \n",
    "#         word = model.get_word_vector(splits[1]);\n",
    "#         feature = ''\n",
    "#         i=0\n",
    "#         for v in word[:20]:\n",
    "#             feature = feature+' '+str(v)\n",
    "# #             feature = feature+' '+'F'+str(i)+':'+str(v)\n",
    "\n",
    "#             i=i+1\n",
    "#         embedding = \"|FastText%s\"%feature;\n",
    "#         strq = \"|FASTTEXT T0:%s U1:%s\"%(word[1],word[1]);\n",
    "        tag = splits[-1]\n",
    "        if tag not in hash:\n",
    "            hash[tag] = c\n",
    "            c+=1\n",
    "\n",
    "        #writer.write('%s 1.0  %s:%s%s %s\\n'%((int(splits[7])+1) + (hash[tag]<<8), int(splits[7]),tag,strw, strp))\n",
    "        # Writing in order\n",
    "        # head, tag_integer mapping, head, tag, word, pos\n",
    "        writer.write('%s %s %s:%s%s %s\\n' % (int(splits[4]), hash[tag], int(splits[4]), tag, strw, strp))\n",
    "\n",
    "    writer.close()\n",
    "print('Value of out of voc tag ',c)\n",
    "print('successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26397"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tag_file_name = '../data/san_tags'\n",
    "word_bag = []\n",
    "orginal_matrix = np.zeros((26397,50))\n",
    "i=0\n",
    "files = ['ud_pos_ner_dp_dev_san','ud_pos_ner_dp_train_san','ud_pos_ner_dp_test_san']\n",
    "for input_file_name in files:\n",
    "    data = open('../data/'+input_file_name).readlines()\n",
    "    for line in data:\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        splits = line.strip().split('\\t')\n",
    "        orginal_matrix[i,:]= model.get_word_vector(splits[1]);\n",
    "        i=i+1\n",
    "        word_text = splits[1]\n",
    "        word_bag.append(word_text)\n",
    "len(word_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08159269,  0.19417113,  0.13765413, -0.45470846, -0.4197349 ,\n",
       "       -0.21231282, -0.41884321, -0.20276827,  0.22137457,  0.13476115,\n",
       "        0.19721089, -0.0968048 ,  0.05417325, -0.42797488,  0.43728924,\n",
       "        0.10927987,  0.38313797, -0.30190808,  0.24237655,  0.13581228,\n",
       "        0.19167817, -0.05475578, -0.15709263, -0.17503366,  0.27312219,\n",
       "        0.16160093,  0.32023454,  0.24088013,  0.1857627 ,  0.05184751,\n",
       "       -0.27082035,  0.64556378, -0.0846865 ,  0.22134319,  0.31643289,\n",
       "       -0.1204012 , -0.62744975, -0.53327149,  0.0697695 , -0.61514646,\n",
       "       -0.2341888 ,  0.20329249,  0.27313218,  0.51035041,  0.29914197,\n",
       "       -0.20607686,  0.51562643,  0.11302631,  0.57586199, -1.17282164])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orginal_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "dimension = 20\n",
    "pca = PCA(n_components=dimension)\n",
    "transformed_matrix = pca.fit_transform(orginal_matrix)\n",
    "# pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.03744129, -0.60063563, -0.44991772, ..., -0.04196214,\n",
       "        -0.08245785, -0.30596154],\n",
       "       [ 1.53274251,  0.3388618 , -0.12079977, ..., -0.14332189,\n",
       "        -0.07115606,  0.03454543],\n",
       "       [-0.65370493, -1.40960605,  0.25744043, ..., -0.03437456,\n",
       "        -0.22628438, -0.06080004],\n",
       "       ...,\n",
       "       [-0.04472763, -1.06349972,  0.05868328, ...,  0.13517823,\n",
       "        -0.14489636,  0.18230696],\n",
       "       [-0.47179015, -1.01882396,  0.09939194, ..., -0.38331316,\n",
       "        -0.09637578,  0.63386845],\n",
       "       [-0.26836749, -0.57620769, -0.14616382, ...,  0.48635555,\n",
       "         1.07135168,  0.35100457]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "writer = open('../models/Dep_san/'+'embedding_size_'+str(dimension)+'.dict','w')\n",
    "for i,word in enumerate(word_bag):\n",
    "    feature = ''\n",
    "    j=0\n",
    "    for v in transformed_matrix[i]:\n",
    "        feature = feature+' '+'F'+str(j)+':'+str(v)\n",
    "        j+=1\n",
    "    embedding = word+\"%s\"%feature;\n",
    "    writer.write('%s\\n' % (embedding))\n",
    "writer.close()\n",
    "print('successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "# tag_file_name = '../data/san_tags'\n",
    "# files = ['ud_pos_ner_dp_dev_san','ud_pos_ner_dp_train_san','ud_pos_ner_dp_test_san']\n",
    "# writer = open('../models/Dep_san/'+'embedding.dict','w')\n",
    "# for input_file_name in files:\n",
    "#     data = open('../data/'+input_file_name).readlines()\n",
    "#     for line in data:\n",
    "#         if line == '\\n':\n",
    "#             continue\n",
    "#         splits = line.strip().split('\\t')\n",
    "#         word = model.get_word_vector(splits[1]);\n",
    "#         feature = ''\n",
    "#         i=0\n",
    "#         for v in word:\n",
    "#             feature = feature+' '+'F'+str(i)+':'+str(v)\n",
    "#             i+=1\n",
    "#         embedding = splits[1]+\"%s\"%feature;\n",
    "     \n",
    "#         writer.write('%s\\n' % (embedding))\n",
    "\n",
    "# writer.close()\n",
    "# print('successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dict of MA because space is creating problem\n",
    "So change all tags to integer mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "tag_dict = dict()\n",
    "for tag in tag_set:\n",
    "    tag_dict[tag] = i\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify evaluate script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: 68.517\n",
      "L: 49.289\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluation script modified from redshift parser\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def pc(num, den):\n",
    "    return (num / float(den+1e-100)) * 100\n",
    "\n",
    "def fmt_acc(label, n, l_corr, u_corr, total_errs):\n",
    "    l_pc = pc(l_corr, n)\n",
    "    u_pc = pc(u_corr, n)\n",
    "    err_pc = pc(n - l_corr, total_errs)\n",
    "    return '%s\\t%d\\t%.3f\\t%.3f\\t%.3f' % (label, n, l_pc, u_pc, err_pc)\n",
    "\n",
    "\n",
    "def gen_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [Token(i, tok_str.split()) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "def gen_note_toks(loc):\n",
    "    sent_strs = open(str(loc)).read().strip().split('\\n\\n')\n",
    "    token = None\n",
    "    i = 0\n",
    "    for sent_str in sent_strs:\n",
    "        # print(sent_str)\n",
    "        tokens = [note_Token(i, tok_str.split('\\t')) for i, tok_str in enumerate(sent_str.split('\\n'))]\n",
    "        for token in tokens:\n",
    "            yield sent_str, token\n",
    "\n",
    "class note_Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        self.label = attrs[-1]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-2])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-1])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        self.head = head\n",
    "        self.pos = attrs[2]\n",
    "        self.word = attrs[1]\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "        \n",
    "class Token(object):\n",
    "    def __init__(self, id_, attrs):\n",
    "        self.id = id_\n",
    "        # CoNLL format\n",
    "        if len(attrs) == 10:\n",
    "            new_attrs = [str(int(attrs[0]) - 1)]\n",
    "            new_attrs.append(attrs[1])\n",
    "            new_attrs.append(attrs[3])\n",
    "            new_attrs.append(str(int(attrs[-4]) - 1))\n",
    "            new_attrs.append(attrs[-3])\n",
    "            attrs = new_attrs\n",
    "        self.label = attrs[-1]\n",
    "        if self.label.lower() == 'root':\n",
    "            self.label = 'ROOT'\n",
    "        try:\n",
    "            head = int(attrs[-2])\n",
    "        except:\n",
    "            try:\n",
    "                self.label = 'P'\n",
    "                head = int(attrs[-1])\n",
    "            except:\n",
    "                print attrs\n",
    "                raise\n",
    "        attrs.pop()\n",
    "        attrs.pop()\n",
    "        self.head = head\n",
    "        self.pos = attrs.pop()\n",
    "        self.word = attrs.pop()\n",
    "        self.dir = 'R' if head >= 0 and head < self.id else 'L'\n",
    "\n",
    "test_loc = 'Dep_san/dep.test.parse'\n",
    "gold_loc = 'sanskrit_treebank/ud_pos_ner_dp_dev_san'\n",
    "n_by_label = defaultdict(lambda: defaultdict(int))\n",
    "u_by_label = defaultdict(lambda: defaultdict(int))\n",
    "l_by_label = defaultdict(lambda: defaultdict(int))\n",
    "N = 0\n",
    "u_nc = 0\n",
    "l_nc = 0\n",
    "for (sst, t), (ss, g) in zip(gen_toks(test_loc), gen_note_toks(gold_loc)):\n",
    "#     not eval_punct and \n",
    "    if g.word in \",.-;:'\\\"!?`{}()[]\":\n",
    "        continue\n",
    "    prev_g = g\n",
    "    prev_t = t\n",
    "    u_c = g.head == t.head\n",
    "    l_c = u_c and g.label.lower() == t.label.lower()\n",
    "    N += 1\n",
    "    l_nc += l_c\n",
    "    u_nc += u_c\n",
    "    n_by_label[g.dir][g.label] += 1\n",
    "    u_by_label[g.dir][g.label] += u_c\n",
    "    l_by_label[g.dir][g.label] += l_c\n",
    "n_l_err = N - l_nc\n",
    "for D in ['L', 'R']:\n",
    "    n_other = 0\n",
    "    l_other = 0\n",
    "    u_other = 0\n",
    "    for label, n in sorted(n_by_label[D].items(), key=lambda i: i[1], reverse=True):\n",
    "        if n == 0:\n",
    "            continue\n",
    "        elif n < 100:\n",
    "            n_other += n\n",
    "            l_other += l_by_label[D][label]\n",
    "            u_other += u_by_label[D][label]\n",
    "        else:\n",
    "            l_corr = l_by_label[D][label]\n",
    "            u_corr = u_by_label[D][label]\n",
    "print 'U: %.3f' % pc(u_nc, N)\n",
    "print 'L: %.3f' % pc(l_nc, N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '2', '6:karwa|w', 'aham', '|p', 'nom.', 'sg.', '*']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = '6 2 6:karwa|w aham |p nom. sg. *'\n",
    "r.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
